# Example GitHub Actions workflow for E2E tests
# Copy this to .github/workflows/e2e.yml in your repository

name: E2E Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'

env:
  RUST_VERSION: 1.75.0
  CARGO_TERM_COLOR: always

jobs:
  # Quick smoke test on every push
  smoke-test:
    name: Smoke Test (Quick)
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - uses: actions/checkout@v3

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: ${{ env.RUST_VERSION }}
          profile: minimal
          override: true

      - name: Cache Cargo
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            target/
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

      - name: Generate test certificates
        run: |
          cd production/certs
          ./generate_certs.sh

      - name: Run cluster setup tests
        run: |
          cd production
          cargo test --package e2e-tests --test cluster_setup -- --ignored --test-threads=1
        env:
          E2E_CERTS_PATH: ${{ github.workspace }}/production/certs
          RUST_LOG: info

  # Full test suite on main branch and PRs
  full-e2e:
    name: Full E2E Suite
    runs-on: ubuntu-latest
    timeout-minutes: 120
    if: github.event_name == 'push' && github.ref == 'refs/heads/main' || github.event_name == 'pull_request'

    strategy:
      matrix:
        test-category:
          - cluster_setup
          - transaction_lifecycle
          - byzantine_scenarios
          - fault_tolerance
          - concurrency
          - network_partition
          - certificate_rotation
      fail-fast: false

    steps:
      - uses: actions/checkout@v3

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: ${{ env.RUST_VERSION }}
          profile: minimal
          override: true

      - name: Cache Cargo
        uses: actions/cache@v3
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            target/
          key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

      - name: Generate test certificates
        run: |
          cd production/certs
          ./generate_certs.sh

      - name: Run E2E tests - ${{ matrix.test-category }}
        run: |
          cd production
          cargo test --package e2e-tests --test ${{ matrix.test-category }} -- --ignored --test-threads=1 --nocapture
        env:
          E2E_CERTS_PATH: ${{ github.workspace }}/production/certs
          RUST_LOG: info

      - name: Cleanup Docker resources
        if: always()
        run: |
          docker ps -a | grep e2e- | awk '{print $1}' | xargs -r docker stop || true
          docker ps -a | grep e2e- | awk '{print $1}' | xargs -r docker rm || true
          docker network ls | grep e2e- | awk '{print $1}' | xargs -r docker network rm || true

  # Performance benchmarks (nightly only)
  benchmarks:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event_name == 'schedule'

    steps:
      - uses: actions/checkout@v3

      - name: Install Rust
        uses: actions-rs/toolchain@v1
        with:
          toolchain: ${{ env.RUST_VERSION }}
          profile: minimal
          override: true

      - name: Generate test certificates
        run: |
          cd production/certs
          ./generate_certs.sh

      - name: Run benchmark tests
        run: |
          cd production
          cargo test --package e2e-tests --test benchmarks -- --ignored --test-threads=1 --nocapture
        env:
          E2E_CERTS_PATH: ${{ github.workspace }}/production/certs
          RUST_LOG: info

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: benchmark-results
          path: production/target/criterion/

  # Report test results
  report:
    name: Test Report
    runs-on: ubuntu-latest
    needs: [smoke-test, full-e2e]
    if: always()

    steps:
      - name: Check test results
        run: |
          if [ "${{ needs.smoke-test.result }}" != "success" ] || [ "${{ needs.full-e2e.result }}" != "success" ]; then
            echo "Some tests failed"
            exit 1
          fi
          echo "All tests passed!"
